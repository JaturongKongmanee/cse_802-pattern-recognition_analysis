\documentclass[11pt]{article}
\usepackage{bookman}
\usepackage[charter]{mathdesign}
\usepackage{amsmath}
\usepackage[colorlinks, urlcolor=blue,linkcolor=red]{hyperref}

\setlength{\textheight}{8.875in} \setlength{\textwidth}{6.875in}
\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{0in}
\setlength{\footskip}{40mm} \setlength{\parindent}{1pc}
\setlength{\oddsidemargin}{-.1875in} 
\setlength{\evensidemargin}{-.1875in}

\newcommand{\myvector}[1]{\mbox{\boldmath$#1$}}

\title{\bf Homework 2}

\author{{\bf \normalsize CSE 802 - Pattern Recognition and Analysis} \\
{\bf \normalsize Total Points: 100} \\
{\bf \normalsize Instructor: Dr. Arun Ross} \\
{\bf \normalsize Due Date: March 1, 12:40pm}}
\date{}

\begin{document}

\maketitle

\noindent \rule{7in}{1pt}

{\em \bf \small Note:

\begin{enumerate}
\item You are permitted to discuss the following questions with others in the class. However, you {\em must} write up your {\em own} solutions to these questions. Any indication to
the contrary will be considered an act of academic dishonesty. Copying from {\em any source} constitutes academic dishonesty. 
\item A neatly typed report is expected (alternately, you can neatly handwrite the report and then scan it). The report, in PDF format, must be uploaded in D2L by March 1, 12:40 pm. Late submissions will not be graded.  In your submission, please include the names of individuals you discussed this homework with and the list of external resources (e.g., websites, other books, articles, etc.) that you used to complete the assignment (if any). 
\item When solving equations or reducing expressions you must explicitly show every step in your computation and/or include the code that was used to perform the computation. Missing steps or code will lead to a deduction of points. 
\item Code developed as part of this assignment must be (a) included as an appendix to your report or inline with your solution, {\bf and} (b) archived in a single zip file and uploaded in D2L. Including the code without the outputs or including the outputs without the code will result in deduction of points.
\end{enumerate}
}
\noindent \rule{7in}{1pt}

\begin{enumerate}

%------------------------------------------------------------------------------

\item Consider a set of 1-dimensional feature values (i.e., points)  pertaining to a class that is available \href{http://www.cse.msu.edu/~rossarun/courses/sp21/cse802/data/hw02_data01.txt}{here}. 
\begin{enumerate}
\item{[3 points]} Plot the histogram of these points using a bin size of 2, i.e., each bin should have a range of 2. 
\item{[4 points]} Compute and report the mean and the biased variance of these points.
\item{[3 points]} Assuming that the given points are generated by an underlying Gaussian distribution, plot the pdf function on the same graph as (a).  
\end{enumerate}

%------------------------------------------------------------------------------

\item{[10 points]} Consider the problem of distinguishing between two classes of fish - $\omega_1$ and $\omega_2$ - based on their length. You can assume that these are the only two classes of fish in nature and that the length of the fish is a discrete integral value! The probability of observing a fish from class $\omega_1$ is 0.6 and the probability of observing a fish from class $\omega_2$ is 0.4. The probability of encountering a fish of length 10 inches, given that it is from class $\omega_1$, is 0.2. Similarly, the probability of encountering a fish of length 10 inches, given that it is from class $\omega_2$, is 0.4. 

(a) Based on this information, compute the probability of encountering a fish of any class of length 10 inches?\\
(b) Based on the Bayes decision rule, to which class will a fish of length 10 inches be assigned to?\\

%------------------------------------------------------------------------------

\item{[15 points]} Consider a 1-dimensional classification problem involving two categories
$\omega_1$ and $\omega_2$ such that $P(\omega_1) = 2/3$ and $P(\omega_2)
= 1/3$. Assume that the classification process can result in one of three 
actions:

$\alpha_1$ -  choose $\omega_1$;\\
$\alpha_2$ -  choose $\omega_2$;\\
$\alpha_3$ - do not classify.

Consider the following loss function, $\lambda$:

$\lambda(\alpha_1|\omega_1) = \lambda(\alpha_2|\omega_2) = 0$;\\
$\lambda(\alpha_2|\omega_1) = \lambda(\alpha_1|\omega_2) = 1$;\\
$\lambda(\alpha_3|\omega_1) = \lambda(\alpha_3|\omega_2) = 1/4$.

For a given feature value $x$, assume that $p(x|\omega_1) = \frac{2-x}{2}$ and
$p(x|\omega_2) = 1/2$. Here, $0 \le x \le 2$. 

Based on the Bayes minimum risk rule, what action will be undertaken when encountering the value $x = 0.5$?

%------------------------------------------------------------------------------

\item Consider a two-class problem with the following class-conditional pdfs (Gaussian density functions):
\[
p(x \mid \omega_1) \sim N(50, 5)
\]
and 
\[
p(x \mid \omega_2) \sim N(30,10).
\]

\begin{enumerate}
\item{\label{item:pdfs}[4 points]} Plot the two class-conditional pdfs in the interval $x$ $\in$ $[10, 125]$ on the same graph.

\item{\label{item:lrs}[4 points]} In another graph, plot the likelihood ratio, $\frac{p(x \mid \omega_1)}{p(x \mid \omega_2)}$, in the range $x$ $\in$ $[10, 125]$.

\item{\label{it:loss1}[5 points]} Recall that the likelihood ratio can be compared against a threshold, say $\eta$, in order to assign a feature value $x$ to one of the two classes, $\omega_1$ or $\omega_2$. Assuming that $\lambda_{11} = \lambda_{22} = 0$, $\lambda_{12} = 2$, $\lambda_{21} = 1$, $P(\omega_1)=0.5$ and $P(\omega_2)=0.5$, what is the value of $\eta$?  Show this threshold in graph \ref{item:lrs} and mark the decision regions corresponding to $\omega_1$ and $\omega_2$. 

\item{\label{it:loss2}[5 points]} Now suppose that $\lambda_{11} = \lambda_{22} = 0$, $\lambda_{12} = 1$, $\lambda_{21} = 2$, $P(\omega_1)=0.5$ and $P(\omega_2)=0.5$, what is the value of $\eta$? Show this threshold also in graph \ref{item:lrs} and mark the decision regions corresponding to $\omega_1$ and $\omega_2$.

\item{[2 points]} Explain in words, the underlying reason for the change in decision regions in \ref{it:loss1} and \ref{it:loss2}. 
\end{enumerate}

%------------------------------------------------------------------------------
\item{[10 points]} In many pattern classification problems, the classifier
is allowed to reject an input pattern by not assigning it to any one of the
$c$ classes. If the cost of rejection is not too high, it may be a desirable
action in some cases. Let
\[
\lambda(\alpha_i\mid \omega_j) = 
\begin{cases}
0, & i = j \quad (i,j = 1,\ldots c)\\
\lambda_r, & i = c+1 \\
\lambda_s, & i \ne j \quad (i,j = 1,\ldots c),\\
\end{cases}
\]
where $\lambda_r$ is the loss incurred for rejecting an input pattern and 
$\lambda_s$ is the loss incurred for misclassifying the input pattern (known
as substitution error). 

\begin{description}
\item{(a)} Show that the minimum risk rule results in the following decision policy.

Assign pattern $\myvector{x}$ to class $\omega_i$ if
$P(\omega_i\mid \myvector{x}) \ge P(\omega_j\mid \myvector{x}) \forall j$ 
{\bf AND} $P(\omega_i\mid \myvector{x}) \ge 1 - \lambda_r/\lambda_s$,
else reject it.

\item{(b)} Explain what happens to the decision policy if $\lambda_r=0$? Similarly, explain what happens to the decision policy if $\lambda_r > \lambda_s$?   
\end{description}
%------------------------------------------------------------------------------

\item{[10 points]} Consider a two-class one-dimensional
classification problem with the following class-conditional
densities:
\[
p(x|\omega_1) = 2 - 2x, \quad x \in [0,1]
\]
\[
p(x|\omega_2) = 2x, \quad x \in [0,1]
\]

(a) Plot these two densities in the same graph.

(b) Let $P(\omega_1) = P(\omega_2) = 1/2$. Assuming a 0-1 loss function, compute the Bayes decision boundary and write down the Bayes decision rule. Mark the decision boundary and decision regions on the figure in (a).

(c) Let $P(\omega_1) = P(\omega_2) = 1/2$. Suppose the loss function is defined as follows: $\lambda_{11}=\lambda_{22}=0$, $\lambda_{12}=2$ and $\lambda_{21}=1$. Compute the Bayes decision boundary and write down the Bayes decision rule. Note that  $\lambda_{ij}$ represents the loss incurred when a sample from class $\omega_j$ is classified as $\omega_i$. Mark the decision boundary and decision regions on the figure in (a).

(d) Intuitively explain why the boundaries in (b) and (c) are different.

%------------------------------------------------------------------------------

\item{[10 points]} Consider a two-class problem with the following class-conditional probability density functions (pdfs):
\[
p(x \mid \omega_1) \sim N(0, \sigma^2)
\]
and 
\[
p(x \mid \omega_2) \sim N(1, \sigma^2).
\]

Show that the threshold, $\tau$, corresponding to the Bayes decision boundary is: 
\[
\tau = \frac{1}{2} - \sigma^2 \ln \left[\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}\right],
\]
where we have assumed that $\lambda_{11} = \lambda_{22} = 0$. 

%------------------------------------------------------------------------------

\item Consider the three-dimensional normal distribution
$p(\myvector{x})$ $\sim$ $N(\myvector{\mu},
\myvector{\Sigma})$, where $\myvector{\mu} = (1, 1, 1)^t$ and
$\myvector{\Sigma} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 5 & 2 \\ 0 & 2
& 5 \end{bmatrix}$. 


\begin{enumerate}
\item{[2 points]} Compute and report the determinant of the covariance matrix, i.e., $\mid \Sigma \mid$.
\item{[2 points]} Compute and report the inverse of the covariance matrix, i.e., $\Sigma^{-1}$.
\item{[3 points]} Compute and report the eigen-vectors and eigen-values of the covariance matrix. 
\item{[3 points]} Compute and report the density value at $(0, 0, 0)^t$ and at $(5,5,5)^t$. 
\item{[2 points]} Compute the Euclidean Distance between $\myvector{\mu}$ and the point $(5,5,5)^t$. 
\item{[3 points]} Compute the Mahalanobis Distance between $\myvector{\mu}$ and the point $(5,5,5)^t$. 

\end{enumerate}

%------------------------------------------------------------------------------


\end{enumerate}

\rule{7in}{1pt}

\end{document}
